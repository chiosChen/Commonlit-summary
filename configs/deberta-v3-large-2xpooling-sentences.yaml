seed : 42
model_name : microsoft/deberta-v3-large
kfold_name : "fold_k_5_seed_42"
selected_folds : [0,1,2,3,4] 
name : deberta-v3-large
exp_name : "dv3l-mp-paragraph--01"  
checkpoints_path : checkpoint/deberta-v3-large/dv3l-mp-sentences--01'  
device : 0

model:
  target : ['grammar_mistake', 'readability','cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar','conventions']
  target_weights : [0.03,0.03,0.18,0.16,0.1,0.16,0.18,0.16]
  max_len : 2048
  max_len_eval : 2048
  loss : FeedbackLoss
  loss_reduction : mean
  sub_loss : RMSELoss
  sub_loss_param : 
    reduction : 'none'
  pretrained_config : 
  num_labels : 8
  model_name : microsoft/deberta-v3-large
  spans : "sentences"
  pooling_params :
    pooling_name: MeanPooling
    params : {}
  spans_pooling_params :
    pooling_name : AttentionHead
    params : {}

optimizer:
  name : optim.AdamW
  params : 
    lr: 4e-6
    betas: [0.9, 0.98]
    eps: 1e-6
    weight_decay: 0.01

scheduler:
  name: cosine
  warmup: 0.04

train_loader:
  batch_size: 1
  drop_last: true
  num_workers: 16
  pin_memory: false
  shuffle: true

val_loader:
  batch_size: 1
  drop_last: false
  num_workers: 16
  pin_memory: false
  shuffle: false

trainer:
  use_amp: true
  epochs: 4
  sample: false
  use_gradient_checkpointing: false
  grad_clip: true
  max_norm: 10

callbacks:
  save : true
  es: false
  patience: 0
  verbose_eval: 1
  epoch_pct_eval: 0.1
  epoch_eval_dist: uniforme
  metric_track: val_loss
  mode: min
  top_k: 1
  start_eval_epoch : 0
