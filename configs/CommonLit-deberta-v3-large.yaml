seed : 42
model_name : microsoft/deberta-v3-large
kfold_name : "fold_k_4_seed_42"
selected_folds : [0,1,2,3]
name : deberta-v3-large
exp_name : "dv3l--01"
checkpoints_path : checkpoint/deberta-v3-large/dv3l--01'
device : 0
n_fold: 4

model:
  target : ['content', 'wording']
  text: ['full_text', 'text']
  max_len : 2048
  max_len_eval : 2048
  loss : RMSELoss
  loss_reduction : mean
  pretrained_config :
  num_labels : 2
  model_name : microsoft/deberta-v3-large
  spans : "sentence"
  pooling_params :
    pooling_name: Attention
    params : {}


optimizer:
  name : optim.AdamW
  params :
    lr: 5e-6
    betas: [0.9, 0.98]
    eps: 1e-6
    weight_decay: 0.01

scheduler:
  name: cosine
  warmup: 0.05

train_loader:
  batch_size: 1
  drop_last: true
  num_workers: 4
  pin_memory: false
  shuffle: true

val_loader:
  batch_size: 1
  drop_last: false
  num_workers: 4
  pin_memory: false
  shuffle: false

trainer:
  use_amp: true
  epochs: 4
  sample: false
  use_gradient_checkpointing: false
  grad_clip: true
  max_norm: 10

wandb:
  use_wandb: true
  api: ''
  project_name: ''

callbacks:
  save : true
  es: false
  patience: 0
  verbose_eval: 1
  epoch_pct_eval: 0.1
  epoch_eval_dist: uniform
  metric_track: val_loss
  mode: min
  top_k: 1
  start_eval_epoch : 0
